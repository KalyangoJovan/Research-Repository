---
title: "CIWR Assignment 3"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
---
# Jovan Kalyango & Thijs Carri√®re
## Part I: Empirical power calculation

You have the following data: two groups, an experimental and a control group, of 50 participants each, that were randomly assigned to two conditions. The values of interest are the test scores on an exam for both groups. The participants in the experimental grouped received a special training on learning strategies prior to an exam. Based on past research you expect that the control group will have a mean of 150 on the test with standard deviation of 15 and that the experimental group will have a mean on the test scores that is 10 points higher than the control group, with the same standard deviation. You plan to perform a t-test on the data and you would like to know the power of the test. You will estimate the power in two ways: using the in-built R function power.t.test and by simulation.

1. Assuming that the data are approximately normal, simulate one sample of data and perform a t-test using the R in-built t-test function. What is the p-value of the test? What is your conclusion?

```{r}
# set seed to get stable results:
set.seed(13)

# Make two groups
group1 <- rnorm(50, mean = 150, sd = 15) # group 1 with mean 150 and sd 15.
group2 <- rnorm(50, mean = 160, sd = 15) # gropu 2 with mean +10 and sd 15.

# Do the in-build t-test
t.test(group1, group2)

# also get a power estimate, given by R
power.t.test(n = 50, delta = 10, sig.level = .05, sd = 15, type = "two.sample", alternative = "two.sided")

```

On this seed, a p-value of .00238 is obtained, what means that the group difference is significant. However, since this is only one simulation, we can not be too sure about this outcome, because every simulation can get other results. To get a more robust conclusion, we should perform far more simulations before we can conclude anything.


2. Now you will approximate the power by simulation. Assume that the data are normally distributed, write a function that generates data and performs a t-test 1000 times and that stores the values of the t-statistic (or the p-value) in a vector. Thinking about the definition of power in terms of rejecting the null hypothesis, how would you obtain an estimate of the power in this situation? Compare your results with those given by power.t.test.

```{r}
# Write the function
my.boot.t.test <- function(n1, n2, mean1, mean2, sd1, sd2, times) {
  
  # first make vectors to store results
  # make names for the columns to extract them later
  row_names <- c(1:times)
  col_names <- "values"
  vec.t.stat <- matrix(rep(0, times), times, 1, dimnames = list(row_names, col_names))
  vec.p.stat <- matrix(rep(0, times), times, 1, dimnames = list(row_names, col_names))
  
  # Then the for loop for the stats
  for(i in 1:times) {
    
    # sample the two groups: can give n, mean and sd in the function to make it useful for multiple usage.
    group1 <- rnorm(n1, mean = mean1, sd = sd1)
    group2 <- rnorm(n2, mean = mean2, sd = sd2)
    
    # Compute t-tests and store its results in the matrix
    vec.t.stat[i] <- t.test(group1, group2)$statistic
    vec.p.stat[i] <- t.test(group1, group2)$p.value
  }
  
  # get 95% limits of the 1000 stats
  ci <- c(.025, .975) 
  CI.t <- quantile(vec.t.stat, ci)
  CI.p <- quantile(vec.p.stat, ci)
  mean.t <- mean(vec.t.stat)
  mean.p <- mean(vec.p.stat)
  
  # get estimate of power: the 'chance on a correct positive'.
  # first take all rows where the p-value is less than .05
  true.p <- vec.p.stat[vec.p.stat[,1] < .05]
  # then take the percentage of how many rows are less than .05
  pow.est <- length(true.p)/times
  
  
  # Print results
  list.out <- list(CI.t, CI.p, mean.t, mean.p, pow.est)
  names(list.out) <- c("95% limits of obtained t-values", "95% limits of obtained p-values", "mean sampled t-value", "mean sampled p-value", "power estimate")
  
  print(list.out)
  
}

# set seed and test the function:
set.seed(13)
my.boot.t.test(n1 = 50, n2 = 50, mean1 = 150, mean2 = 160, sd1 = 15, sd2 = 15, times = 1000)

# do a power.t.test
# delta as what is expected in the population.
power.t.test(n = 50, delta = 10, sd = 15, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
```

If power is seen as the percentage of simulations that find a significant result, we get a power estimate of .916. 
When the power of the test is evaluated with the R-function, we obtain a power of .910. These values differ a little bit, but are more or less of the same magnitude. However, when more simulations are used for the bootstrap, the power estimate approaches the in-build power estimate more (10.000 simulations: power estimate = .908, 100.000 simulations: power estimate = .910). So with enough simulations, we will probably find the power estimate R has given.


# Part II: Resampling techniques

First get the data set for this part of the assignment
```{r}
# Make the two groups
CSFI <- c(2, 5, 5, 6, 6, 7, 8, 9)
TFI <- c(1, 1, 2, 3, 3, 4, 5, 7, 7, 8)
```


1. Choose an appropriate resampling technique and make a function that performs a hypothesis test based on the t-statistic produced by the standard t-test function. Use the in-built sample() function to perform the resampling of the data. Include relevant statistics in your function and give the function a clear structure (choose sensible input arguments, organize the output in an efficient way, add comments, etc). Show the results of your resampling technique on the Flight instruction data.

```{r}
# We made a function for a resample-test that can do both bootstrap and permutation test. We would normally use the bootstrap, because then there are more possible samples and this function can be used for smaller groups as well.

# Funtion for resample t-test: sided.test is either '1' or '2', default '1'. For sampling.method, choose between 'bootstrap' or 'permutation.test': bootstrap default
My.resample.test <- function(grp1, grp2, nboots, sided.test = 1, sampling.method = "bootstrap") {
  
  # Get groupsizes
  n1 <- length(grp1)
  n2 <- length(grp2)
  Ntot <- n1+n2
  
  # Get observed statistic
  observed.t <- t.test(grp1, grp2)$statistic 
  
  # generate sampling distribution either bootstrap or permutation
  # Permutation test
  if(sampling.method == "permutation.test") {
    pooled_data2 <- c(grp1, grp2)
  t.twotest <- c() 
  set.seed(123)
  for (i in 1: nboots) {
    tot.samp <- sample(pooled_data2, Ntot, replace = F)
    sample1 <- tot.samp[1:n1]
    sample2 <- tot.samp[n1:Ntot]
    t.twotest[i] <- t.test(sample1, sample2)$statistic
  }

  # bootstrap
  } else {
  pooled_data2 <- c(grp1, grp2)
  t.twotest <- c() 
  set.seed(123)
  for (i in 1: nboots) {
    sample1 <- sample(pooled_data2, n1, replace = T)
    sample2 <- sample(pooled_data2, n2, replace = T)
    t.twotest[i] <- t.test(sample1, sample2)$statistic
  }}
  
  # Generate p-value > either one-sided or two-sided
  if(sided.test == 2) { 
    p.value <- mean(abs(t.twotest) > abs(observed.t))
  } else {
    p.value <- mean(t.twotest > abs(observed.t))
  }
 
  # generate confidence interval
  confi <- quantile(t.twotest, probs=c(0.025, 0.975))
 
  # descriptives by group for the output
  group.mean1 <- mean(grp1)  # mean of sample 1
  group.mean2 <- mean(grp2)   # mean of sample 2
  s1 <- var(grp1)      # variance of sample1
  s2 <- var(grp2)    # variance of sample 2
  means <- cbind(group.mean1, group.mean2)
  vars <- cbind(s1,s2)
  samplSize <- cbind(n1, n2)
  
  # assemble the output of the function
  outcomes <- list(observed.t, p.value, confi, means, vars, samplSize)
  names(outcomes) <- c("observed T.statistic", "obtained p.value", "95%-sampling interval", "means", "group variances", "group sizes")
  return(outcomes)
}

# test the function with bootstrap and 2-sided test.
My.resample.test(grp1 = CSFI, grp2 = TFI, nboots = 1000, sided.test = 2, sampling.method = "bootstrap")
```


2. How would you program the drawing of random samples when you do not want to use the sample() function? Modify the function you have programmed in (1) in such a way that it does not use the sample() function anymore (hint: you will have to work with indices). Run your new function and show that you can obtain the same results as in (1).

```{r}
# For this resampling without "sample()" we only used the bootstrap method
# Funtion for resample t-test: sided.test is either '1' or '2', default '1'. 
My.res.test.no.samp <- function(grp1, grp2, nboots, sided.test = 1) {
  
  # Get groupsizes
  n1 <- length(grp1)
  n2 <- length(grp2)
  Ntot <- n1+n2
  
  # Get observed statistic
  observed.t <- t.test(grp1, grp2)$statistic 
  
  # generate sampling distribution either bootstrap or permutation
  # bootstrap
  pooled_data2 <- c(grp1, grp2)
  t.twotest <- c() 
  set.seed(123)
  
  # For every bootstrap 2 'samples' are created.
  # Every sample is filled with random indices, obtained with 'runif()'
  for (i in 1:nboots) {
  sample.1 <- c(rep(0, n1))
  sample.2 <- c(rep(0, n2)) 
  
  # filling of sample 1
  for (i in 1:n1) {
    indice.1 <- round(runif(1, min = .5, max = (Ntot + .5)))
    sample.1[i] <- pooled_data2[indice.1]
  }
 
  # Filling of sample 2
  for (i in 1:n2) {
    indice.2 <- round(runif(1, min = .5, max = (Ntot + .5)))
    sample.2[i] <- pooled_data2[indice.2] 
  }
  t.twotest <- c(t.twotest, t.test(sample.1, sample.2)$statistic)
  
  }
  
  # Generate p-value > either one-sided or two-sided
  if(sided.test == 2) { 
    p.value <- mean(abs(t.twotest) > abs(observed.t))
  } else {
    p.value <- mean(t.twotest > abs(observed.t))
  }
 
  # generate confidence interval
  confi <- quantile(t.twotest, probs=c(0.025, 0.975))
 
  # descriptives by group for the output
  group.mean1 <- mean(grp1)  # mean of sample 1
  group.mean2 <- mean(grp2)   # mean of sample 2
  s1 <- var(grp1)      # variance of sample1
  s2 <- var(grp2)    # variance of sample 2
  means <- cbind(group.mean1, group.mean2)
  vars <- cbind(s1,s2)
  samplSize <- cbind(n1, n2)
  
  # assemble the output of the function
  outcomes <- list(observed.t, p.value, confi, means, vars, samplSize)
  names(outcomes) <- c("observed T.statistic", "obtained p.value", "95%-sampling interval", "means", "group variances", "group sizes")
  return(outcomes)
}

# test the function with bootstrap and 2-sided test.
My.res.test.no.samp(grp1 = CSFI, grp2 = TFI, nboots = 1000, sided.test = 2)

```
Answer Q2:
The obtained results differ a little bit from the last question. This is because now, we take pseudo-random numbers for the indices, which still has randomness and therefore isn't exactly the same as question 1. Even with the same seed, we get a different result, since we sample indices now. However, the number of samples we took was not that high. When we take more samples (for example 100,000) the p-values are almost the same (both .105).
We looked really hard for a way to get samples without even taking random indices. However, then your simulation would end up with systematic samples, what is not the goal of a simulation. A possible way to 'sample' without randomness, is by taking every possible permutation. However, then the number of simulations would be enormous and when only taking 1000, the systematic approach in permutating doesn't have the character of a good simulation.


3. Which resampling technique(s) would you use, if you would be interested in estimation instead of hypothesis testing? Make an R function that performs resampling for estimation purposes using the mean difference between two groups from the t-test function. Your function should produce all relevant aspects of estimation, and you should discuss these aspects and put them in perspective. Again use the Flight instruction data to show the results of your function.

```{r}
# For estimation we would choose for the bootstrap resampling method. We would choose for this because this method would work with really small samples where the permutationtest wouldn't work because of too few possible permutations. Also, the bootstrap deals better with differences in variance when estimating, which is often the case for a t-test.

# Write a function: bootstrap for estimation purposes.
Boot.est <- function(grp1, grp2, nboots) {
  obs.meandiff <- abs((mean(grp1) - mean(grp2))) # take the observed mean difference
  
  # search for mean difference of nboots and relevant objects, like group sizes
  samp.meandiff<- matrix(rep(0,nboots),nboots,1) 
  boot.t.val <- matrix(rep(0,nboots),nboots,1)
  size.1 <- length(grp1)
  size.2 <- length(grp2)
  set.seed(123)
  
  # sample bootstrap samples and store those meandifferences in an object
  for(i in 1:nboots) {
    sample1 <- sample(grp1, size.1, replace = TRUE)
    sample2 <- sample(grp2, size.2, replace = TRUE)
    samp.meandiff[i] <- abs(mean(sample1) - (mean(sample2)))
  }
  
  # Compute bias of the estimate
  bias2 <- mean(samp.meandiff)-obs.meandiff## bias in mean difference 
  
  # Compute confidence interval for the estimate obtained by the resampling. 
  CI2 <- quantile(samp.meandiff, probs = c(0.025, 0.975))
  
  # compute the standard deviation of the bootstrap distribution
  stan.err <- sd(samp.meandiff)
  
  # Organise output in a list
  output <- list(CI2, bias2, obs.meandiff, mean(samp.meandiff), stan.err)
  names(output) <- c("CI of estimation of mean difference", "bias of estimation", "observed statistic", "Mean sampled statistic", "Standard error of the bootstrap distribution")
  return(output)
}

# Test the estimation function with the given data
Boot.est(CSFI, TFI, 1000)
```

Answer Q3:
Our estimate showed that the true mean difference of this population lays with 95% certainty in the confidence interval of 0.15 and 3.85125. This confidence interval is computed with the percentile method.
The computed bias shows how much our mean estimate differs from the true observed mean difference. Bias has to be as low as possible, ideally even 0. The more bootstrap samples used, the smaller the bias will get. N = 1000, as used in our example, is quite high and therefore, the estimation has quite low bias.
The computed standard error of the sampling distribution tells something about how spread the data is. When using a high number of bootstrap samples, 2 standard errors below and above the mean will be the same as the obtained values of the 95%-percentile confidence interval. This is the case with our output as well.
